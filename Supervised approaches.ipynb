{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-building phase: supervised approaches\n",
    "- [Preprocessing](#Data-Preprocessing)\n",
    "    - [Scaling Data](#Scaling-Data)\n",
    "    - [Splitting Data](#Splitting-Data)\n",
    "- [Supervised Models](#Supervised-Models)\n",
    "    - [Random Forest](#Random-Forest-with-Water-100)\n",
    "    - [Perceptron](#Perceptron)\n",
    "    - [SVM](#Support-Vector-Machines-(SVM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from prep import *\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from  sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preprocessing the data we make use of the *prep* function, which simultaneously allows us both to deal with the missing values, giving us the choice of removing them, or partially removing them by replacing the remainder with the mean or median of the corresponding variable, and to scale the data, with the possibility of choosing the method by which to scale such data from all the scalers in scikit-learn, by default the MinMaxScaler is set. The function then takes as input a pandas DataFrame and outputs a numpy ndarray containing the cleaned data from the previous dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to generate two datasets: the first by eliminating all observations having at least one component with a missing value, the second by eliminating only 50 percent of those observations. Eventually we will train each model using both datasets and collect their metrics in order to assess whether on average such a reduction in missing values to be eliminated (thus replacing the missing part) resulted in any benefit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data\n",
    "To observe if there are difference in performance of the models  trained on a dataset with  all the Nan values imputed and in the models trained on a dataset with only a percentage of Nan values imputed, we have  created 2 dataset:\n",
    "- one where we imputed all the Nan values (Water 100 dataset)\n",
    "- one where we imputed only the half Nan values deleting the remain part of observations (Water 50 dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset size:  (3276, 10) - type:  <class 'pandas.core.frame.DataFrame'>\n",
      "cleaned dataset with 50% of missing values removed:  (2644, 10) - type:  <class 'numpy.ndarray'>\n",
      "cleaned dataset with no missing values removed:  (3276, 10) - type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Setting random state for each model\n",
    "water = pd.read_csv('dataset/drinking_water_potability.csv')\n",
    "water50 = prep(\n",
    "    data = water,\n",
    "    target='Potability',\n",
    "    axis='obs',\n",
    "    perc=50,\n",
    "    fill_method='mean',\n",
    "    scaler= StandardScaler()\n",
    ")\n",
    "water100 = prep(\n",
    "    data = water,\n",
    "    target='Potability',\n",
    "    axis='obs',\n",
    "    perc=0,\n",
    "    fill_method='mean',\n",
    "    scaler= StandardScaler()\n",
    ")\n",
    "print('original dataset size: ', water.shape, '- type: ', type(water))\n",
    "print('cleaned dataset with 50% of missing values removed: ', np.shape(water50), '- type: ', type(water50))\n",
    "print('cleaned dataset with no missing values removed: ', np.shape(water100), '- type: ', type(water100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "At this point we proceed to divide the dataset into train set, validation set and test set. To do this, we make use of the *train_test_split()* function of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure repeatability test\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE SPLITTING: \n",
      "\n",
      "X shape:  (2644, 9)\n",
      "y shape:  (2644,)\n",
      "\n",
      "AFTER SPLITTING: \n",
      "X_train shape:  (1850, 9)\n",
      "y_train shape:  (1850,)\n",
      "X_test shape:  (794, 9)\n",
      "y_test shape:  (794,)\n",
      "X_val shape:  (397, 9)\n",
      "y_val shape:  (397,)\n"
     ]
    }
   ],
   "source": [
    "#Water 50\n",
    "X_train50, X_val50, X_test50, y_train50, y_val50, y_test50=split(df = water50,\n",
    "                                                    target_index = 9,\n",
    "                                                    validation = True,\n",
    "                                                    perc_train = 0.7,\n",
    "                                                    random_seed = random_seed,\n",
    "                                                    verbose=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE SPLITTING: \n",
      "\n",
      "X shape:  (3276, 9)\n",
      "y shape:  (3276,)\n",
      "\n",
      "AFTER SPLITTING: \n",
      "X_train shape:  (2293, 9)\n",
      "y_train shape:  (2293,)\n",
      "X_test shape:  (983, 9)\n",
      "y_test shape:  (983,)\n",
      "X_val shape:  (491, 9)\n",
      "y_val shape:  (491,)\n"
     ]
    }
   ],
   "source": [
    "#Water 100\n",
    "X_train100, X_val100, X_test100, y_train100, y_val100, y_test100=split(df = water100,\n",
    "                                                    target_index = 9,\n",
    "                                                    perc_train = 0.7,\n",
    "                                                    random_seed = random_seed,\n",
    "                                                    verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Models\n",
    "The Supervised models that we chose are the following:\n",
    "- **Random Forest**\n",
    "- **Gradient Boosting**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest** with Water 100\n",
    "In this part of the report we will train the Random Forest on Water100, that one with all the Nan values imputed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 80.04%.\n"
     ]
    }
   ],
   "source": [
    "# Training base model\n",
    "rf_base = RandomForestClassifier(random_state = random_seed)\n",
    "rf_base.fit(X_train100, y_train100)\n",
    "#Computing the base model accuracy\n",
    "rf_base_accuracy = evaluate(rf_base, X_val100, y_val100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Random Forest with Random Search and Cross Validation\n",
    " First of all we will start searching the best configuration of hyperparameters with a random search choosing among \n",
    " thousand combinations of hyperparameters 100 random combinations, after this first step we will focus more with a grid \n",
    "search around the best combinations found with the random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#Defining Random Forest Claasifier\n",
    "rf = RandomForestClassifier(random_state = random_seed)\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we can see there are many hyperparameter that we can tune, but for the moment we will focus more only on  the most importants\n",
    "\n",
    "[Info about RandomForest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
       " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False],\n",
       " 'criterion': ['gini', 'entropy', 'log_loss']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2',None]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Metrics  to measure the quality of a split.\n",
    "criterion =['gini', 'entropy', 'log_loss']\n",
    "# Create the random grid\n",
    "params_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion':criterion}\n",
    "params_grid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually if we used all the possible combinations we should train the random forest 25920 times without considering the cross validation for each combinations, that would require a computational time too high, for this reason initially we will use the random search using only 100 random combinations among those available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#\n",
    "rf_random_search = RandomizedSearchCV(estimator = rf, param_distributions = params_grid,\n",
    "                                n_iter = 100, cv = 3, verbose=3, random_state=random_seed, n_jobs = -1)\n",
    "# Fit the  model\n",
    "#\n",
    "#rf_random.fit(X_train100, y_train100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store and Load Models\n",
    "To avoid  traininig each time the models we will save after every training of the models the results inside *Models folder* using pickle library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving\n",
    "#pickle.dump(rf_random_search, open('Models/Random_Forest_rs_w100.pkl', 'wb'))\n",
    "#Loading\n",
    "rf_random_search=pickle.load(open('Models/Random_Forest_rs_w100.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1400,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 10,\n",
       " 'criterion': 'log_loss',\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing base model with tuned model with Random Search\n",
    "To check if we are going in right direction we will compare the base model without tuning of parameters with the tuned model, comparing the accuracy of both models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pepee\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:425: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 80.45%.\n",
      "The tuned model had an improvement of 0.51%.\n"
     ]
    }
   ],
   "source": [
    "# Training tuned model\n",
    "rf_random =  RandomForestClassifier(bootstrap = True,criterion='log_loss',max_depth=10,max_features='auto',\n",
    "                                      min_samples_leaf=1,min_samples_split=10,n_estimators=1400,\n",
    "                                      random_state=random_seed)\n",
    "rf_random.fit(X_train100,y_train100)\n",
    "#Computing the tuned model accuracy                                 \n",
    "rf_random_accuracy = evaluate(rf_random, X_val100, y_val100)\n",
    "\n",
    "print('The tuned model had an improvement of {:0.2f}%.'.format( 100 * (rf_random_accuracy - rf_base_accuracy) / rf_base_accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Random Forest with Grid Search and Cross Validation\n",
    "Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Random Forest Claasifier\n",
    "rf = RandomForestClassifier(random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1400,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 10,\n",
       " 'criterion': 'log_loss',\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "params_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'criterion':['log_loss'],\n",
    "    'max_depth': [5,10,15],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'min_samples_split': [5,10,15],\n",
    "    'n_estimators': [1300,1400,1500]\n",
    "}\n",
    "# Instantiate the grid search model\n",
    "rf_grid_search = GridSearchCV(estimator = rf, param_grid = params_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 1)\n",
    "# Fit the grid search to the data\n",
    "#rf_grid.fit(X_train100, y_train100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving\n",
    "#pickle.dump(rf_grid_search, open('Models/Random_Forest_rs_w100.pkl', 'wb'))\n",
    "#Loading\n",
    "rf_grid_search = pickle.load(open('Models/Random_Forest_gs_w100.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'log_loss',\n",
       " 'max_depth': 10,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 15,\n",
       " 'n_estimators': 1500}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing tuned models \n",
    "To check if we are going in right direction we will compare the base model without tuning of parameters with the tuned model comparing the accuracy of both models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pepee\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:425: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 80.86%.\n",
      "The tuned model with grid search had an improvement of 1.02%. respect the base model\n",
      "The tuned model with grid search had an improvement of 0.51%. respect the tuned  model with random search\n"
     ]
    }
   ],
   "source": [
    "# Training tuned model\n",
    "rf_final =  RandomForestClassifier(bootstrap = True,criterion='log_loss',max_depth=10,max_features='auto',\n",
    "                                      min_samples_leaf=2,min_samples_split=15,n_estimators=1500,\n",
    "                                      random_state=random_seed)\n",
    "rf_final.fit(X_train100,y_train100)\n",
    "#Computing the tuned model accuracy                                 \n",
    "rf_grid_accuracy = evaluate(rf_final, X_val100, y_val100)\n",
    "\n",
    "print('The tuned model with grid search had an improvement of {:0.2f}%. respect the base model'\n",
    "                        .format( 100 * (rf_grid_accuracy - rf_base_accuracy) / rf_base_accuracy))\n",
    "\n",
    "print('The tuned model with grid search had an improvement of {:0.2f}%. respect the tuned  model with random search'\n",
    "                        .format( 100 * (rf_grid_accuracy - rf_random_accuracy) / rf_random_accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting with Water 100\n",
    "In this part of the report we will train the Gradient Boosting on Water100, that one with all the Nan values imputed.\n",
    "\n",
    "In boosting, the individual models are not built on completely random subsets of data and features but sequentially by putting more weight on instances with wrong predictions and high errors. The general idea behind this is that instances, which are hard to predict correctly (“difficult” cases) will be focused on during learning, so that the model learns from past mistakes. When we train each ensemble on a subset of the training set, we also call this Stochastic Gradient Boosting, which can help improve generalizability of our model.The gradient is used to minimize a loss function, similar to how Neural Nets utilize gradient descent to optimize (“learn”) weights. \n",
    "\n",
    "Source: [link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 78.82%.\n"
     ]
    }
   ],
   "source": [
    "# Training base model\n",
    "gb_base = GradientBoostingClassifier( random_state = random_seed)\n",
    "gb_base.fit(X_train100, y_train100)\n",
    "#Computing the base model accuracy\n",
    "gb_base_accuracy = evaluate(gb_base, X_val100, y_val100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Gradient Boosting with Random Search and Cross Validation\n",
    "\n",
    "First of all we will start searching the best configuration of hyperparameters with a random search choosing among \n",
    " thousand combinations of hyperparameters 100 random combinations, after this first step we will focus more with a grid \n",
    "search around the best combinations found with the random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'log_loss',\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_iter_no_change': None,\n",
      " 'random_state': 42,\n",
      " 'subsample': 1.0,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#Defining Random Forest Claasifier\n",
    "gb = GradientBoostingClassifier(random_state = random_seed)\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(gb.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': ['friedman_mse', 'squared_error'],\n",
      " 'learning_rate': [0.001, 0.01, 0.1],\n",
      " 'loss': ['log_loss', 'exponential'],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "# The loss function to be optimized.\n",
    "loss = ['log_loss','exponential']\n",
    "# Learning rate shrinks the contribution of each tree by learning_rate\n",
    "learning_rate=[0.001,0.01,0.1]\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2',None]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Metrics  to measure the quality of a split.\n",
    "criterion =['friedman_mse','squared_error']\n",
    "# Create the random grid\n",
    "params_grid = {'loss': loss,\n",
    "               'learning_rate':learning_rate,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'criterion':criterion}\n",
    "pprint(params_grid)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually if we used all the possible combinations we should train the Gradient Boosting 103000 times without considering the cross validation for each combinations, that would require a computational time too high, for this reason initially we will use the random search using only 100 random combinations among those available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#\n",
    "gb_random_search = RandomizedSearchCV(estimator = gb, param_distributions = params_grid,\n",
    "                                n_iter = 100, cv = 3, verbose=3, random_state=random_seed, n_jobs = -1)\n",
    "# Fit the  model\n",
    "#\n",
    "#gb_random_search.fit(X_train100, y_train100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store and Load Models\n",
    "To avoid  traininig each time the models we will save after every training of the models the results inside *Models folder* using pickle library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving\n",
    "#pickle.dump(gb_random_search.best_params_, open('Models/Gradient_Boosting_rs_w100.pkl', 'wb'))\n",
    "\n",
    "#Loading\n",
    "gb_params_best_random=pickle.load(open('Models/Gradient_Boosting_rs_w100.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1600,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 20,\n",
       " 'loss': 'exponential',\n",
       " 'learning_rate': 0.01,\n",
       " 'criterion': 'friedman_mse'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_params_best_random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing base model with tuned model with Random Search\n",
    "To check if we are going in right direction we will compare the base model without tuning of parameters with the tuned model, comparing the accuracy of both models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 79.23%.\n",
      "The tuned model with grid search had an improvement of 0.52%. respect the base model\n"
     ]
    }
   ],
   "source": [
    "# Training tuned model\n",
    "gb_random =  GradientBoostingClassifier(loss='exponential',max_depth=20,max_features='sqrt',\n",
    "                                      min_samples_leaf=1,min_samples_split=2,n_estimators=1600,\n",
    "                                      learning_rate=0.01,criterion='friedman_mse',\n",
    "                                      random_state=random_seed)\n",
    "gb_random.fit(X_train100,y_train100)\n",
    "#Computing the tuned model accuracy                                 \n",
    "gb_random_accuracy = evaluate(gb_random, X_val100, y_val100)\n",
    "\n",
    "print('The tuned model with grid search had an improvement of {:0.2f}%. respect the base model'\n",
    "                    .format( 100 * (gb_random_accuracy - gb_base_accuracy) / gb_base_accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search with Cross Validation\n",
    "Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1600,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 20,\n",
       " 'loss': 'exponential',\n",
       " 'learning_rate': 0.01,\n",
       " 'criterion': 'friedman_mse'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_params_best_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingClassifier(random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;], &#x27;learning_rate&#x27;: [0.01],\n",
       "                         &#x27;loss&#x27;: [&#x27;exponential&#x27;], &#x27;max_depth&#x27;: [15, 20, 25],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 3],\n",
       "                         &#x27;min_samples_split&#x27;: [1, 2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [1500, 1600, 1700]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingClassifier(random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;], &#x27;learning_rate&#x27;: [0.01],\n",
       "                         &#x27;loss&#x27;: [&#x27;exponential&#x27;], &#x27;max_depth&#x27;: [15, 20, 25],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 3],\n",
       "                         &#x27;min_samples_split&#x27;: [1, 2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [1500, 1600, 1700]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'criterion': ['friedman_mse'], 'learning_rate': [0.01],\n",
       "                         'loss': ['exponential'], 'max_depth': [15, 20, 25],\n",
       "                         'max_features': ['sqrt'],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [1, 2, 3],\n",
       "                         'n_estimators': [1500, 1600, 1700]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "params_grid = {\n",
    "    'loss': ['exponential'],\n",
    "    'learning_rate': [0.01],\n",
    "    'criterion':['friedman_mse'],\n",
    "    'max_depth': [15,20,25],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'min_samples_split': [1,2,3],\n",
    "    'n_estimators': [1500,1600,1700]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "gb_grid_search = GridSearchCV(estimator = gb, param_grid = params_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 1)\n",
    "                          # Fit the grid search to the data\n",
    "gb_grid_search.fit(X_train100, y_train100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving\n",
    "#pickle.dump(gb_grid_search.best_params_, open('Models/Gradient_Boosting_gs_w100.pkl', 'wb'))\n",
    "\n",
    "#Loading\n",
    "gb_params_best_random=pickle.load(open('Models/Gradient_Boosting_gs_w100.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'friedman_mse',\n",
       " 'learning_rate': 0.01,\n",
       " 'loss': 'exponential',\n",
       " 'max_depth': 25,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 1,\n",
       " 'n_estimators': 1600}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_params_best_random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing tuned models \n",
    "To check if we are going in right direction we will compare the base model without tuning of parameters with the tuned model comparing the accuracy of both models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 79.84%.\n",
      "The tuned model with grid search had an improvement of 1.29%. respect the base model\n",
      "The tuned model with grid search had an improvement of 0.77%. respect the tuned  model with random search\n"
     ]
    }
   ],
   "source": [
    "# Training tuned model\n",
    "gb_final =  GradientBoostingClassifier(loss='exponential',max_depth=25,max_features='sqrt',\n",
    "                                      min_samples_leaf=3,min_samples_split=1,n_estimators=1600,\n",
    "                                      learning_rate=0.01,criterion='friedman_mse',\n",
    "                                      random_state=random_seed)\n",
    "gb_final.fit(X_train100,y_train100)\n",
    "#Computing the tuned model accuracy                                 \n",
    "gb_final_accuracy = evaluate(gb_final, X_val100, y_val100)\n",
    "\n",
    "print('The tuned model with grid search had an improvement of {:0.2f}%. respect the base model'\n",
    "                    .format( 100 * (gb_final_accuracy - gb_base_accuracy) / gb_base_accuracy))\n",
    "\n",
    "print('The tuned model with grid search had an improvement of {:0.2f}%. respect the tuned  model with random search'\n",
    "                        .format( 100 * (gb_final_accuracy - gb_random_accuracy) / gb_random_accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers. The *perceptron* is suitable for large scale learning. By default:\n",
    "- It does not require a learning rate.\n",
    "- It is not regularized (penalized).\n",
    "- It updates its model only on mistakes.\n",
    "\n",
    "source: [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'alpha': 0.0001,\n",
      " 'class_weight': None,\n",
      " 'early_stopping': False,\n",
      " 'eta0': 1.0,\n",
      " 'fit_intercept': False,\n",
      " 'l1_ratio': 0.15,\n",
      " 'max_iter': 1000,\n",
      " 'n_iter_no_change': 5,\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': 42,\n",
      " 'shuffle': True,\n",
      " 'tol': 0.001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 1,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "perc_model = Perceptron(\n",
    "    penalty = 'l2', # The penalty (aka regularization term) to be used.\n",
    "    alpha = 0.0001, # Constant that multiplies the regularization term if regularization is used.\n",
    "    fit_intercept=False, # Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.\n",
    "    shuffle= True, # Whether or not the training data should be shuffled after each epoch.\n",
    "    verbose = 1, # The verbosity level.\n",
    "    random_state = random_seed, # Used to shuffle the training data, when shuffle is set to True. Pass an int for reproducible output across multiple function calls.\n",
    ")\n",
    "\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(perc_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'sklearn.linear_model._sgd_fast._memoryviewslice' object has no attribute 'nonzero'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_591/1507463196.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_more_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m         return self._fit(\n\u001b[0m\u001b[1;32m    897\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         self._partial_fit(\n\u001b[0m\u001b[1;32m    686\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    627\u001b[0m             )\n\u001b[1;32m    628\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             self._fit_binary(\n\u001b[0m\u001b[1;32m    630\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(self, X, y, alpha, C, sample_weight, learning_rate, max_iter)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;34m\"\"\"Fit a binary classifier on X and y.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         coef, intercept, n_iter_ = fit_binary(\n\u001b[0m\u001b[1;32m    715\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit_binary\u001b[0;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     coef, intercept, average_coef, average_intercept, n_iter_ = _plain_sgd(\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/linear_model/_sgd_fast.pyx\u001b[0m in \u001b[0;36msklearn.linear_model._sgd_fast._plain_sgd\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'sklearn.linear_model._sgd_fast._memoryviewslice' object has no attribute 'nonzero'"
     ]
    }
   ],
   "source": [
    "perc_model.fit(X = X_train100, y = y_train100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution Models\n",
    "\n",
    "| Models              | Test Accuracy | Test Recall | Test Precision | F1 Score |\n",
    "|---------------------|---------------|-------------|----------------|----------|\n",
    "| Logistic Regression |               |             |                |          |\n",
    "| Random Forest       |               |             |                |          |\n",
    "| K-NN                |               |             |                |          |\n",
    "| Orazio              |               |             |                |          |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
